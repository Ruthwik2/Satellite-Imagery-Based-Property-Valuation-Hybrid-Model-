{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b798967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append('../src')\n",
    "from data_fetcher import SatelliteImageFetcher\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1541c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6445825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'target_col': 'price',\n",
    "    'lat_col': 'lat',\n",
    "    'lon_col': 'long',\n",
    "    'image_size': 224,\n",
    "    'seed': 42,\n",
    "    'test_size': 0.2,\n",
    "    'use_log_target': True,\n",
    "}\n",
    "\n",
    "# Paths - UPDATE THESE\n",
    "DATA_DIR = '../data'\n",
    "IMAGE_DIR = '../data/mapbox_images'\n",
    "OUTPUT_DIR = '../outputs'\n",
    "TRAIN_PATH = f'{DATA_DIR}/train.xlsx'\n",
    "TEST_PATH = f'{DATA_DIR}/test.xlsx'\n",
    "\n",
    "np.random.seed(CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efb15d0",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8319186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_excel(TRAIN_PATH, engine='openpyxl')\n",
    "test_df = pd.read_excel(TEST_PATH, engine='openpyxl')\n",
    "\n",
    "# Clean column names\n",
    "train_df.columns = [c.strip() for c in train_df.columns]\n",
    "test_df.columns = [c.strip() for c in test_df.columns]\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe8a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTrain columns:\")\n",
    "print(train_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c64108",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7f54fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8a77a0",
   "metadata": {},
   "source": [
    "## 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fdf68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39dc14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(train_df[CONFIG['target_col']], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Price')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Price Distribution')\n",
    "\n",
    "axes[1].hist(np.log1p(train_df[CONFIG['target_col']]), bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[1].set_xlabel('Log Price')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Log Price Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03297e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "missing = train_df.isnull().sum()\n",
    "missing_pct = (missing / len(train_df)) * 100\n",
    "missing_df = pd.DataFrame({'Missing': missing, 'Percent': missing_pct})\n",
    "missing_df[missing_df['Missing'] > 0].sort_values('Percent', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b215e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation with target\n",
    "numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if CONFIG['target_col'] in numeric_cols:\n",
    "    correlations = train_df[numeric_cols].corr()[CONFIG['target_col']].drop(CONFIG['target_col']).sort_values(key=abs, ascending=False)\n",
    "    print(\"\\nCorrelations with target:\")\n",
    "    print(correlations.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8839c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "corr_matrix = train_df[numeric_cols].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e87fdbe",
   "metadata": {},
   "source": [
    "## 3. Geographic Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40ee0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['lat_col'] in train_df.columns and CONFIG['lon_col'] in train_df.columns:\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    scatter = plt.scatter(\n",
    "        train_df[CONFIG['lon_col']], \n",
    "        train_df[CONFIG['lat_col']], \n",
    "        c=np.log1p(train_df[CONFIG['target_col']]),\n",
    "        cmap='viridis',\n",
    "        alpha=0.6,\n",
    "        s=10\n",
    "    )\n",
    "    plt.colorbar(scatter, label='Log Price')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.title('Property Locations Colored by Price')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ff449a",
   "metadata": {},
   "source": [
    "## 4. Image Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271a4414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count available images\n",
    "if os.path.exists(IMAGE_DIR):\n",
    "    image_files = [f for f in os.listdir(IMAGE_DIR) if f.endswith('.png')]\n",
    "    print(f\"Total images: {len(image_files)}\")\n",
    "    print(f\"Training samples: {len(train_df)}\")\n",
    "    print(f\"Coverage: {len(image_files)/len(train_df)*100:.1f}%\")\n",
    "else:\n",
    "    print(f\"Image directory not found: {IMAGE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc50f860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images\n",
    "if os.path.exists(IMAGE_DIR) and len(image_files) > 0:\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    sample_images = np.random.choice(image_files, min(8, len(image_files)), replace=False)\n",
    "    \n",
    "    for ax, img_file in zip(axes.flatten(), sample_images):\n",
    "        img_path = os.path.join(IMAGE_DIR, img_file)\n",
    "        img = Image.open(img_path)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(img_file[:15])\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Sample Satellite Images', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c280887",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93fc0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature columns\n",
    "exclude_cols = {CONFIG['target_col'], 'date', 'id', CONFIG['lat_col'], CONFIG['lon_col']}\n",
    "feature_cols = [c for c in train_df.columns if c not in exclude_cols and train_df[c].dtype in ['int64', 'float64']]\n",
    "\n",
    "print(f\"Feature columns ({len(feature_cols)}):\")\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature statistics\n",
    "train_df[feature_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4caed37",
   "metadata": {},
   "source": [
    "## 6. Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ac099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(train_df, test_size=CONFIG['test_size'], random_state=CONFIG['seed'])\n",
    "\n",
    "train_image_ids = train_data.index.tolist()\n",
    "val_image_ids = val_data.index.tolist()\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1614216f",
   "metadata": {},
   "source": [
    "## 7. Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e5cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "X_train = preprocessor.fit_transform(train_data[feature_cols])\n",
    "X_val = preprocessor.transform(val_data[feature_cols])\n",
    "X_test = preprocessor.transform(test_df[feature_cols])\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df20074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare target variable\n",
    "if CONFIG['use_log_target']:\n",
    "    y_train = np.log1p(train_data[CONFIG['target_col']].values)\n",
    "    y_val = np.log1p(val_data[CONFIG['target_col']].values)\n",
    "    y_train_original = train_data[CONFIG['target_col']].values\n",
    "    y_val_original = val_data[CONFIG['target_col']].values\n",
    "    print(\"Using log-transformed target\")\n",
    "else:\n",
    "    y_train = train_data[CONFIG['target_col']].values\n",
    "    y_val = val_data[CONFIG['target_col']].values\n",
    "    y_train_original = y_train\n",
    "    y_val_original = y_val\n",
    "    print(\"Using original target\")\n",
    "\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab1f417",
   "metadata": {},
   "source": [
    "## 8. Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93536215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save preprocessor\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "joblib.dump(preprocessor, f'{OUTPUT_DIR}/preprocessor.joblib')\n",
    "print(f\"Saved preprocessor to {OUTPUT_DIR}/preprocessor.joblib\")\n",
    "\n",
    "# Save feature columns\n",
    "joblib.dump(feature_cols, f'{OUTPUT_DIR}/feature_cols.joblib')\n",
    "print(f\"Saved feature columns to {OUTPUT_DIR}/feature_cols.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705683d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed arrays\n",
    "np.save(f'{OUTPUT_DIR}/X_train.npy', X_train)\n",
    "np.save(f'{OUTPUT_DIR}/X_val.npy', X_val)\n",
    "np.save(f'{OUTPUT_DIR}/X_test.npy', X_test)\n",
    "np.save(f'{OUTPUT_DIR}/y_train.npy', y_train)\n",
    "np.save(f'{OUTPUT_DIR}/y_val.npy', y_val)\n",
    "np.save(f'{OUTPUT_DIR}/y_train_original.npy', y_train_original)\n",
    "np.save(f'{OUTPUT_DIR}/y_val_original.npy', y_val_original)\n",
    "np.save(f'{OUTPUT_DIR}/train_image_ids.npy', np.array(train_image_ids))\n",
    "np.save(f'{OUTPUT_DIR}/val_image_ids.npy', np.array(val_image_ids))\n",
    "\n",
    "print(f\"\\nSaved all preprocessed data to {OUTPUT_DIR}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6099639",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"Log target: {CONFIG['use_log_target']}\")\n",
    "print(\"\\n‚úÖ Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50382cbd",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1008d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_excel('../data/train.xlsx', engine='openpyxl')\n",
    "test_df = pd.read_excel('../data/test.xlsx', engine='openpyxl')\n",
    "\n",
    "# Clean column names\n",
    "train_df.columns = [c.strip() for c in train_df.columns]\n",
    "test_df.columns = [c.strip() for c in test_df.columns]\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"\\nTraining columns: {list(train_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605a6125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "print(\"=\" * 50)\n",
    "print(\"TRAINING DATA INFO\")\n",
    "print(\"=\" * 50)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b511f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First few rows\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561bdb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7951f7f1",
   "metadata": {},
   "source": [
    "## 2. Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d1e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "missing_train = train_df.isnull().sum()\n",
    "missing_test = test_df.isnull().sum()\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Train Missing': missing_train,\n",
    "    'Train %': (missing_train / len(train_df) * 100).round(2),\n",
    "    'Test Missing': missing_test,\n",
    "    'Test %': (missing_test / len(test_df) * 100).round(2)\n",
    "})\n",
    "\n",
    "missing_df[missing_df['Train Missing'] > 0].sort_values('Train Missing', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253faf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "missing_pct = (train_df.isnull().sum() / len(train_df) * 100).sort_values(ascending=True)\n",
    "missing_pct[missing_pct > 0].plot(kind='barh', ax=ax, color='coral')\n",
    "ax.set_xlabel('Missing Percentage (%)')\n",
    "ax.set_title('Missing Values in Training Data')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e823cc13",
   "metadata": {},
   "source": [
    "## 3. Target Variable Analysis (Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c5559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(train_df['price'], bins=50, color='steelblue', edgecolor='white')\n",
    "axes[0].set_xlabel('Price ($)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Price Distribution')\n",
    "axes[0].axvline(train_df['price'].median(), color='red', linestyle='--', label=f'Median: ${train_df[\"price\"].median():,.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Log-transformed histogram\n",
    "axes[1].hist(np.log1p(train_df['price']), bins=50, color='seagreen', edgecolor='white')\n",
    "axes[1].set_xlabel('Log(Price)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Log-Transformed Price Distribution')\n",
    "\n",
    "# Box plot\n",
    "axes[2].boxplot(train_df['price'], vert=True)\n",
    "axes[2].set_ylabel('Price ($)')\n",
    "axes[2].set_title('Price Box Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/price_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Price statistics\n",
    "print(\"\\nüìä Price Statistics:\")\n",
    "print(f\"   Mean:   ${train_df['price'].mean():,.2f}\")\n",
    "print(f\"   Median: ${train_df['price'].median():,.2f}\")\n",
    "print(f\"   Std:    ${train_df['price'].std():,.2f}\")\n",
    "print(f\"   Min:    ${train_df['price'].min():,.2f}\")\n",
    "print(f\"   Max:    ${train_df['price'].max():,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6219558a",
   "metadata": {},
   "source": [
    "## 4. Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c3342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns for correlation\n",
    "numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Numeric columns: {numeric_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbd7f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with price\n",
    "if 'price' in train_df.columns:\n",
    "    correlations = train_df[numeric_cols].corr()['price'].drop('price').sort_values(ascending=False)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    colors = ['green' if x > 0 else 'red' for x in correlations.values]\n",
    "    correlations.plot(kind='barh', ax=ax, color=colors)\n",
    "    ax.set_xlabel('Correlation with Price')\n",
    "    ax.set_title('Feature Correlations with Property Price')\n",
    "    ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/price_correlations.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüîù Top 5 Positive Correlations:\")\n",
    "    print(correlations.head())\n",
    "    print(\"\\nüîª Top 5 Negative Correlations:\")\n",
    "    print(correlations.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b969aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "corr_matrix = train_df[numeric_cols].corr()\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r', \n",
    "            center=0, ax=ax, square=True, linewidths=0.5,\n",
    "            annot_kws={'size': 8})\n",
    "ax.set_title('Feature Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/correlation_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca8b068",
   "metadata": {},
   "source": [
    "## 5. Geospatial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f328e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for lat/long columns\n",
    "lat_col = 'lat'\n",
    "lon_col = 'long'\n",
    "\n",
    "if lat_col in train_df.columns and lon_col in train_df.columns:\n",
    "    print(f\"Latitude range: {train_df[lat_col].min():.4f} to {train_df[lat_col].max():.4f}\")\n",
    "    print(f\"Longitude range: {train_df[lon_col].min():.4f} to {train_df[lon_col].max():.4f}\")\n",
    "    \n",
    "    # Geographic distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Scatter plot of locations\n",
    "    scatter = axes[0].scatter(train_df[lon_col], train_df[lat_col], \n",
    "                              c=np.log1p(train_df['price']), cmap='viridis', \n",
    "                              alpha=0.5, s=10)\n",
    "    axes[0].set_xlabel('Longitude')\n",
    "    axes[0].set_ylabel('Latitude')\n",
    "    axes[0].set_title('Property Locations (colored by log price)')\n",
    "    plt.colorbar(scatter, ax=axes[0], label='Log(Price)')\n",
    "    \n",
    "    # 2D histogram / density\n",
    "    h = axes[1].hist2d(train_df[lon_col], train_df[lat_col], bins=50, cmap='YlOrRd')\n",
    "    axes[1].set_xlabel('Longitude')\n",
    "    axes[1].set_ylabel('Latitude')\n",
    "    axes[1].set_title('Property Density Heatmap')\n",
    "    plt.colorbar(h[3], ax=axes[1], label='Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/geospatial_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Latitude/Longitude columns not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cdc30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price by geographic region (using grid)\n",
    "if lat_col in train_df.columns and lon_col in train_df.columns:\n",
    "    # Create geographic bins\n",
    "    train_df['lat_bin'] = pd.cut(train_df[lat_col], bins=10, labels=False)\n",
    "    train_df['lon_bin'] = pd.cut(train_df[lon_col], bins=10, labels=False)\n",
    "    \n",
    "    # Average price by grid cell\n",
    "    price_grid = train_df.groupby(['lat_bin', 'lon_bin'])['price'].mean().unstack()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(price_grid, cmap='YlGnBu', ax=ax, annot=True, fmt='.0f')\n",
    "    ax.set_xlabel('Longitude Bin')\n",
    "    ax.set_ylabel('Latitude Bin')\n",
    "    ax.set_title('Average Property Price by Geographic Grid')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/price_by_location.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Clean up temp columns\n",
    "    train_df.drop(['lat_bin', 'lon_bin'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e50f2b2",
   "metadata": {},
   "source": [
    "## 6. Satellite Image Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57b80bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image directory path (pre-downloaded images)\n",
    "IMAGE_DIR = '/Users/ruthwik/Downloads/mapbox_images'\n",
    "\n",
    "# Check for downloaded images\n",
    "if os.path.exists(IMAGE_DIR):\n",
    "    cached_images = [f for f in os.listdir(IMAGE_DIR) if f.endswith('.png')]\n",
    "    print(f\"Found {len(cached_images)} satellite images in {IMAGE_DIR}\")\n",
    "    print(f\"Sample images: {sorted(cached_images)[:5]}\")\n",
    "else:\n",
    "    print(f\"Image directory not found: {IMAGE_DIR}\")\n",
    "\n",
    "# Auto-detect image naming convention\n",
    "if 'id' in train_df.columns:\n",
    "\n",
    "    test_id = train_df.iloc[0]['id']    return os.path.join(IMAGE_DIR, f'img_{row_or_idx}.png')\n",
    "\n",
    "    test_by_id = os.path.exists(os.path.join(IMAGE_DIR, f'img_{int(test_id)}.png'))        return os.path.join(IMAGE_DIR, f'img_{int(row_or_idx)}.png')\n",
    "\n",
    "else:    elif USE_ID_FOR_IMAGES and isinstance(row_or_idx, (int, float)):\n",
    "\n",
    "    test_by_id = False        return os.path.join(IMAGE_DIR, f'img_{int(row_or_idx[\"id\"])}.png')\n",
    "\n",
    "test_by_idx = os.path.exists(os.path.join(IMAGE_DIR, 'img_0.png'))    if USE_ID_FOR_IMAGES and isinstance(row_or_idx, pd.Series) and 'id' in row_or_idx.index:\n",
    "\n",
    "    \"\"\"Get image path - use ID column if available, otherwise row index.\"\"\"\n",
    "\n",
    "USE_ID_FOR_IMAGES = test_by_iddef get_image_path(row_or_idx):\n",
    "\n",
    "print(f\"\\n\\u2705 Images by ID: {test_by_id}, by index: {test_by_idx}\")\n",
    "print(f\"\\u27a1\\ufe0f Using {'ID column' if USE_ID_FOR_IMAGES else 'row index'} for image lookup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48db0d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample satellite images with their prices\n",
    "def show_sample_images(df, n_samples=9, figsize=(15, 15)):\n",
    "    \"\"\"Display satellite images for random samples with price labels.\"\"\"\n",
    "    # Get random sample indices\n",
    "    sample_indices = np.random.choice(len(df), size=min(n_samples, len(df)), replace=False)\n",
    "    \n",
    "    n_cols = 3\n",
    "    n_rows = (len(sample_indices) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for plot_idx, data_idx in enumerate(sample_indices):\n",
    "        row = df.iloc[data_idx]\n",
    "        lat, lon = row.get(lat_col, 0), row.get(lon_col, 0)\n",
    "        price = row.get('price', 'N/A')\n",
    "        \n",
    "        # Get image by ID (not row index!)\n",
    "        if USE_ID_FOR_IMAGES and 'id' in df.columns:\n",
    "            img_id = row['id']\n",
    "            img_path = os.path.join(IMAGE_DIR, f'img_{int(img_id)}.png')\n",
    "        else:\n",
    "            img_path = os.path.join(IMAGE_DIR, f'img_{data_idx}.png')\n",
    "        \n",
    "        if os.path.exists(img_path):\n",
    "            img = Image.open(img_path)\n",
    "            axes[plot_idx].imshow(img)\n",
    "        else:\n",
    "            # Show placeholder\n",
    "            axes[plot_idx].text(0.5, 0.5, f'Image not found\\n{os.path.basename(img_path)}', \n",
    "                          ha='center', va='center', fontsize=10)\n",
    "            axes[plot_idx].set_facecolor('lightgray')\n",
    "        \n",
    "        if isinstance(price, (int, float)):\n",
    "            axes[plot_idx].set_title(f'Price: ${price:,.0f}\\n({lat:.4f}, {lon:.4f})', fontsize=10)\n",
    "        else:\n",
    "            axes[plot_idx].set_title(f'({lat:.4f}, {lon:.4f})', fontsize=10)\n",
    "        axes[plot_idx].axis('off')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(sample_indices), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('Sample Satellite Images with Property Prices', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/sample_satellite_images.png', dpi=150, bbox_inches='tight')\n",
    "\n",
    "    plt.show()show_sample_images(train_df, n_samples=9)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521fefad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare high vs low price properties - FIXED for ID-based image lookup\n",
    "def compare_price_extremes(df, n_each=4):\n",
    "    \"\"\"Compare satellite images of highest vs lowest priced properties.\"\"\"\n",
    "    valid_df = df.dropna(subset=[lat_col, lon_col, 'price'])\n",
    "    \n",
    "    # Get rows of highest and lowest priced properties\n",
    "    high_rows = valid_df.nlargest(n_each, 'price')\n",
    "    low_rows = valid_df.nsmallest(n_each, 'price')\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n_each, figsize=(4*n_each, 8))\n",
    "    \n",
    "    # High price properties\n",
    "    for plot_idx, (_, row) in enumerate(high_rows.iterrows()):\n",
    "        # Get image by ID (not row index!)\n",
    "        if USE_ID_FOR_IMAGES and 'id' in df.columns:\n",
    "            img_path = os.path.join(IMAGE_DIR, f'img_{int(row[\"id\"])}.png')\n",
    "        else:\n",
    "            img_path = os.path.join(IMAGE_DIR, f'img_{plot_idx}.png')\n",
    "        \n",
    "        if os.path.exists(img_path):\n",
    "            img = Image.open(img_path)\n",
    "            axes[0, plot_idx].imshow(img)\n",
    "        else:\n",
    "            axes[0, plot_idx].text(0.5, 0.5, 'N/A', ha='center', va='center')\n",
    "            axes[0, plot_idx].set_facecolor('lightgray')\n",
    "        \n",
    "        axes[0, plot_idx].set_title(f'${row[\"price\"]:,.0f}', fontsize=11, color='green')\n",
    "        axes[0, plot_idx].axis('off')\n",
    "    \n",
    "    # Low price properties  \n",
    "    for plot_idx, (_, row) in enumerate(low_rows.iterrows()):\n",
    "        # Get image by ID (not row index!)\n",
    "        if USE_ID_FOR_IMAGES and 'id' in df.columns:\n",
    "            img_path = os.path.join(IMAGE_DIR, f'img_{int(row[\"id\"])}.png')\n",
    "        else:\n",
    "            img_path = os.path.join(IMAGE_DIR, f'img_{plot_idx}.png')\n",
    "        \n",
    "        if os.path.exists(img_path):\n",
    "            img = Image.open(img_path)\n",
    "            axes[1, plot_idx].imshow(img)\n",
    "        else:\n",
    "            axes[1, plot_idx].text(0.5, 0.5, 'N/A', ha='center', va='center')\n",
    "            axes[1, plot_idx].set_facecolor('lightgray')\n",
    "        \n",
    "        axes[1, plot_idx].set_title(f'${row[\"price\"]:,.0f}', fontsize=11, color='red')\n",
    "        axes[1, plot_idx].axis('off')\n",
    "    \n",
    "    axes[0, 0].set_ylabel('HIGH\\nPRICE', fontsize=12, rotation=0, ha='right', va='center')\n",
    "    axes[1, 0].set_ylabel('LOW\\nPRICE', fontsize=12, rotation=0, ha='right', va='center')\n",
    "    \n",
    "    plt.suptitle('Satellite Images: Highest vs Lowest Priced Properties', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/high_vs_low_price_images.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "compare_price_extremes(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc2f182",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43752010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"Create engineered features from existing columns.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Example features (adjust based on your actual columns)\n",
    "    if 'sqft_living' in df.columns and 'sqft_lot' in df.columns:\n",
    "        df['living_lot_ratio'] = df['sqft_living'] / (df['sqft_lot'] + 1)\n",
    "    \n",
    "    if 'sqft_living' in df.columns and 'bedrooms' in df.columns:\n",
    "        df['sqft_per_bedroom'] = df['sqft_living'] / (df['bedrooms'] + 1)\n",
    "    \n",
    "    if 'bathrooms' in df.columns and 'bedrooms' in df.columns:\n",
    "        df['bath_bed_ratio'] = df['bathrooms'] / (df['bedrooms'] + 1)\n",
    "    \n",
    "    if 'yr_built' in df.columns:\n",
    "        df['age'] = 2026 - df['yr_built']\n",
    "    \n",
    "    if 'yr_renovated' in df.columns and 'yr_built' in df.columns:\n",
    "        df['is_renovated'] = (df['yr_renovated'] > 0).astype(int)\n",
    "        df['years_since_renovation'] = np.where(\n",
    "            df['yr_renovated'] > 0,\n",
    "            2026 - df['yr_renovated'],\n",
    "            2026 - df['yr_built']\n",
    "        )\n",
    "    \n",
    "    if 'grade' in df.columns and 'condition' in df.columns:\n",
    "        df['quality_score'] = df['grade'] * df['condition']\n",
    "    \n",
    "    if 'sqft_above' in df.columns and 'sqft_basement' in df.columns:\n",
    "        df['has_basement'] = (df['sqft_basement'] > 0).astype(int)\n",
    "        df['basement_ratio'] = df['sqft_basement'] / (df['sqft_above'] + 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "train_featured = engineer_features(train_df)\n",
    "test_featured = engineer_features(test_df)\n",
    "\n",
    "# Show new columns\n",
    "new_cols = set(train_featured.columns) - set(train_df.columns)\n",
    "print(f\"New engineered features: {new_cols}\")\n",
    "\n",
    "if new_cols:\n",
    "    print(\"\\nEngineered features statistics:\")\n",
    "    display(train_featured[list(new_cols)].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7f16ec",
   "metadata": {},
   "source": [
    "## 8. Data Quality Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2342209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final data quality summary\n",
    "print(\"=\" * 60)\n",
    "print(\"üìã DATA QUALITY SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Dataset Sizes:\")\n",
    "print(f\"   Training samples: {len(train_df):,}\")\n",
    "print(f\"   Test samples: {len(test_df):,}\")\n",
    "print(f\"   Features: {len(train_df.columns) - 1}\")\n",
    "\n",
    "print(f\"\\nüí∞ Target Variable (Price):\")\n",
    "print(f\"   Range: ${train_df['price'].min():,.0f} - ${train_df['price'].max():,.0f}\")\n",
    "print(f\"   Mean: ${train_df['price'].mean():,.0f}\")\n",
    "print(f\"   Median: ${train_df['price'].median():,.0f}\")\n",
    "\n",
    "print(f\"\\nüó∫Ô∏è Geographic Coverage:\")\n",
    "print(f\"   Latitude: {train_df[lat_col].min():.4f} to {train_df[lat_col].max():.4f}\")\n",
    "print(f\"   Longitude: {train_df[lon_col].min():.4f} to {train_df[lon_col].max():.4f}\")\n",
    "\n",
    "print(f\"\\nüñºÔ∏è Satellite Images:\")\n",
    "if os.path.exists(cache_dir):\n",
    "    n_cached = len([f for f in os.listdir(cache_dir) if f.endswith('.png')])\n",
    "    print(f\"   Cached images: {n_cached}\")\n",
    "else:\n",
    "    print(f\"   Cached images: 0 (run data_fetcher.py)\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è Missing Values:\")\n",
    "missing_total = train_df.isnull().sum().sum()\n",
    "print(f\"   Total missing values: {missing_total}\")\n",
    "print(f\"   Columns with missing data: {(train_df.isnull().sum() > 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c912ada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data for training\n",
    "train_featured.to_pickle('../data/train_processed.pkl')\n",
    "test_featured.to_pickle('../data/test_processed.pkl')\n",
    "print(\"‚úÖ Processed data saved to data/train_processed.pkl and data/test_processed.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff5bfc4",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù Key Insights\n",
    "\n",
    "Document your key findings here after running the analysis:\n",
    "\n",
    "1. **Price Distribution**: [Your observation]\n",
    "2. **Top Correlated Features**: [Your observation]\n",
    "3. **Geographic Patterns**: [Your observation]\n",
    "4. **Visual Features**: [Your observation about satellite images]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
