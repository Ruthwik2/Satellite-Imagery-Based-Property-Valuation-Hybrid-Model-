{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b70b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION - KAGGLE PATHS\n",
    "# ============================================\n",
    "import os\n",
    "\n",
    "IS_KAGGLE = os.path.exists('/kaggle')\n",
    "print(f\"Running on Kaggle: {IS_KAGGLE}\")\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    # Your dataset paths (based on your Kaggle upload structure)\n",
    "    DATASET_PATH = '/kaggle/input/satellite-property-data'\n",
    "    \n",
    "    TRAIN_PATH = f'{DATASET_PATH}/train(1).xlsx'\n",
    "    TEST_PATH = f'{DATASET_PATH}/test2.xlsx'\n",
    "    \n",
    "    # Images are nested: mapbox_images (1)/mapbox_images/\n",
    "    IMAGE_DIR = f'{DATASET_PATH}/mapbox_images (1)/mapbox_images'\n",
    "    \n",
    "    OUTPUT_DIR = '/kaggle/working'\n",
    "else:\n",
    "    # Local paths\n",
    "    TRAIN_PATH = 'data/train.xlsx'\n",
    "    TEST_PATH = 'data/test.xlsx'\n",
    "    IMAGE_DIR = '/Users/user/Downloads/mapbox_images'\n",
    "    OUTPUT_DIR = 'outputs'\n",
    "\n",
    "print(f\"‚úÖ Train: {TRAIN_PATH} - exists: {os.path.exists(TRAIN_PATH)}\")\n",
    "print(f\"‚úÖ Test: {TEST_PATH} - exists: {os.path.exists(TEST_PATH)}\")\n",
    "print(f\"‚úÖ Images: {IMAGE_DIR} - exists: {os.path.exists(IMAGE_DIR)}\")\n",
    "\n",
    "# Count images\n",
    "if os.path.exists(IMAGE_DIR):\n",
    "    n_images = len([f for f in os.listdir(IMAGE_DIR) if f.endswith('.png')])\n",
    "    print(f\"‚úÖ Found {n_images} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc1f0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (if needed on Kaggle)\n",
    "# !pip install openpyxl xgboost -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d617ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bba2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - IMPROVED\n",
    "CONFIG = {\n",
    "    'target_col': 'price',\n",
    "    'lat_col': 'lat',\n",
    "    'lon_col': 'long',\n",
    "    'image_size': 224,\n",
    "    'seed': 42,\n",
    "    'test_size': 0.2,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 15,           # Epochs for fine-tuning\n",
    "    'lr': 5e-4,             # Lower LR for fine-tuning\n",
    "    'use_log_target': True, # Train on log(price) - often helps!\n",
    "}\n",
    "\n",
    "np.random.seed(CONFIG['seed'])\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "print(f\"‚úÖ Using log-transformed target: {CONFIG['use_log_target']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa45345",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d0b0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_excel(TRAIN_PATH, engine='openpyxl')\n",
    "test_df = pd.read_excel(TEST_PATH, engine='openpyxl')\n",
    "\n",
    "train_df.columns = [c.strip() for c in train_df.columns]\n",
    "test_df.columns = [c.strip() for c in test_df.columns]\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"\\nColumns: {list(train_df.columns)}\")\n",
    "\n",
    "# CRITICAL: Check what images actually exist\n",
    "print(\"\\nüîç Diagnosing image naming...\")\n",
    "sample_images = sorted(os.listdir(IMAGE_DIR))[:10]\n",
    "print(f\"First 10 images: {sample_images}\")\n",
    "\n",
    "# Check if 'id' column exists and matches image names\n",
    "if 'id' in train_df.columns:\n",
    "    print(f\"\\nTrain ID range: {train_df['id'].min()} to {train_df['id'].max()}\")\n",
    "    # Check if first few IDs match image files\n",
    "    for i in range(min(5, len(train_df))):\n",
    "        row_id = train_df.iloc[i]['id']\n",
    "        img_by_id = f'img_{int(row_id)}.png'\n",
    "        img_by_idx = f'img_{i}.png'\n",
    "        id_exists = os.path.exists(os.path.join(IMAGE_DIR, img_by_id))\n",
    "        idx_exists = os.path.exists(os.path.join(IMAGE_DIR, img_by_idx))\n",
    "        print(f\"Row {i}: id={row_id} | img_by_id={img_by_id} exists={id_exists} | img_by_idx={img_by_idx} exists={idx_exists}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c9a2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick EDA\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f2ac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(train_df['price'], bins=50, color='steelblue', edgecolor='white')\n",
    "axes[0].set_xlabel('Price ($)')\n",
    "axes[0].set_title('Price Distribution')\n",
    "axes[0].axvline(train_df['price'].median(), color='red', linestyle='--', label=f'Median: ${train_df[\"price\"].median():,.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(np.log1p(train_df['price']), bins=50, color='seagreen', edgecolor='white')\n",
    "axes[1].set_xlabel('Log(Price)')\n",
    "axes[1].set_title('Log-Transformed Price')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/price_distribution.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced98428",
   "metadata": {},
   "source": [
    "## 2. Sample Satellite Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35e1a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Images are named by ROW INDEX (img_0.png to img_16165.png)\n",
    "# NOT by the ID column (which has numbers like 9117000170)\n",
    "\n",
    "USE_ID_FOR_IMAGES = False  # Images use row index, NOT ID column!\n",
    "\n",
    "def get_image_path(idx):\n",
    "    \"\"\"Get image path by row index.\"\"\"\n",
    "    return os.path.join(IMAGE_DIR, f'img_{idx}.png')\n",
    "\n",
    "print(f\"‚úÖ Images are named by ROW INDEX (img_0.png, img_1.png, ...)\")\n",
    "print(f\"‚úÖ Image range: img_0.png to img_16165.png\")\n",
    "print(f\"‚û°Ô∏è USE_ID_FOR_IMAGES = {USE_ID_FOR_IMAGES}\")\n",
    "\n",
    "# Verify first few images exist\n",
    "for i in range(3):\n",
    "    exists = os.path.exists(get_image_path(i))\n",
    "    print(f\"   img_{i}.png exists: {exists}\")\n",
    "\n",
    "# Display sample images\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "sample_indices = np.random.choice(len(train_df), 9, replace=False)\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    row = train_df.iloc[idx]\n",
    "    img_path = get_image_path(idx)  # Use row index!\n",
    "    \n",
    "    if os.path.exists(img_path):\n",
    "        img = Image.open(img_path)\n",
    "        axes[i].imshow(img)\n",
    "    else:\n",
    "        axes[i].text(0.5, 0.5, f'No Image\\n{os.path.basename(img_path)}', ha='center', va='center')\n",
    "        axes[i].set_facecolor('lightgray')\n",
    "    \n",
    "    axes[i].set_title(f'Price: ${row[\"price\"]:,.0f}', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Satellite Images with Prices', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/sample_images.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da42cd",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babc56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns (exclude non-features)\n",
    "exclude_cols = {CONFIG['target_col'], 'date', 'id', CONFIG['lat_col'], CONFIG['lon_col']}\n",
    "feature_cols = [c for c in train_df.columns if c not in exclude_cols and train_df[c].dtype in ['int64', 'float64']]\n",
    "print(f\"Feature columns ({len(feature_cols)}): {feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d7ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation split\n",
    "train_data, val_data = train_test_split(train_df, test_size=CONFIG['test_size'], random_state=CONFIG['seed'])\n",
    "\n",
    "# IMPORTANT: Use ORIGINAL ROW INDICES for image lookup (img_0.png, img_1.png, ...)\n",
    "# The DataFrame index preserves original row numbers even after split\n",
    "train_image_ids = train_data.index.tolist()  # Original row indices\n",
    "val_image_ids = val_data.index.tolist()      # Original row indices\n",
    "\n",
    "print(f\"Training: {len(train_data)}, Validation: {len(val_data)}\")\n",
    "print(f\"Sample train image indices: {train_image_ids[:5]}\")\n",
    "print(f\"‚úÖ Using row INDEX for image lookup (img_0.png, img_1.png, ...)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fd7cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "preprocessor = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "X_train = preprocessor.fit_transform(train_data[feature_cols])\n",
    "X_val = preprocessor.transform(val_data[feature_cols])\n",
    "X_test = preprocessor.transform(test_df[feature_cols])\n",
    "\n",
    "# Use log-transformed target for better training\n",
    "if CONFIG['use_log_target']:\n",
    "    y_train = np.log1p(train_data[CONFIG['target_col']].values)\n",
    "    y_val = np.log1p(val_data[CONFIG['target_col']].values)\n",
    "    y_train_original = train_data[CONFIG['target_col']].values\n",
    "    y_val_original = val_data[CONFIG['target_col']].values\n",
    "    print(\"‚úÖ Using log1p(price) as target - will transform back for evaluation\")\n",
    "else:\n",
    "    y_train = train_data[CONFIG['target_col']].values\n",
    "    y_val = val_data[CONFIG['target_col']].values\n",
    "    y_train_original = y_train\n",
    "    y_val_original = y_val\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, X_val: {X_val.shape}, X_test: {X_test.shape}\")\n",
    "print(f\"Target range: {y_train.min():.2f} to {y_train.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860f34c8",
   "metadata": {},
   "source": [
    "## 4. Model 1: XGBoost (Tabular Only) - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e53ca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=CONFIG['seed'],\n",
    "    early_stopping_rounds=50,\n",
    "    eval_metric='rmse',\n",
    "    tree_method='hist',  # Faster on Kaggle\n",
    "    device='cuda' if DEVICE == 'cuda' else 'cpu'\n",
    ")\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b1f144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\nüìä {model_name}:\")\n",
    "    print(f\"   RMSE:  ${rmse:,.2f}\")\n",
    "    print(f\"   R¬≤:    {r2:.4f}\")\n",
    "    \n",
    "    return {'model': model_name, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "# XGBoost predictions (on log scale if use_log_target=True)\n",
    "xgb_pred_raw = xgb_model.predict(X_val)\n",
    "\n",
    "# Convert back from log scale for evaluation\n",
    "if CONFIG['use_log_target']:\n",
    "    xgb_pred = np.expm1(xgb_pred_raw)  # Convert log(price) back to price\n",
    "    xgb_results = evaluate_model(y_val_original, xgb_pred, 'XGBoost (Tabular Only)')\n",
    "else:\n",
    "    xgb_pred = xgb_pred_raw\n",
    "    xgb_results = evaluate_model(y_val, xgb_pred, 'XGBoost (Tabular Only)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e44a78",
   "metadata": {},
   "source": [
    "## 5. Model 2: Hybrid Model (Tabular + Satellite Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d7935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class - IMPROVED with better augmentation\n",
    "class PropertyDataset(Dataset):\n",
    "    def __init__(self, X_tabular, y=None, image_ids=None, train_mode=True):\n",
    "        self.X_tab = X_tabular.astype(np.float32)\n",
    "        self.y = y.astype(np.float32) if y is not None else None\n",
    "        self.image_ids = image_ids\n",
    "        \n",
    "        if train_mode:\n",
    "            # More aggressive augmentation for training\n",
    "            self.transform = T.Compose([\n",
    "                T.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "                T.RandomHorizontalFlip(0.5),\n",
    "                T.RandomVerticalFlip(0.5),  # Satellite images can be flipped vertically\n",
    "                T.RandomRotation(15),        # Small rotation\n",
    "                T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = T.Compose([\n",
    "                T.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        \n",
    "        self.blank = Image.new('RGB', (CONFIG['image_size'], CONFIG['image_size']), (128, 128, 128))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_tab)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx] if self.image_ids is not None else idx\n",
    "        img_path = os.path.join(IMAGE_DIR, f'img_{int(img_id)}.png')\n",
    "        \n",
    "        if os.path.exists(img_path):\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "            except:\n",
    "                img = self.blank\n",
    "        else:\n",
    "            img = self.blank\n",
    "        \n",
    "        img_tensor = self.transform(img)\n",
    "        tab_tensor = torch.from_numpy(self.X_tab[idx])\n",
    "        \n",
    "        if self.y is None:\n",
    "            return img_tensor, tab_tensor\n",
    "        return img_tensor, tab_tensor, torch.tensor(self.y[idx])\n",
    "\n",
    "print(\"‚úÖ Dataset with improved augmentation: RandomVerticalFlip, Rotation, ColorJitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Model - STABLE version with FROZEN CNN\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, tabular_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN (ResNet18) - FREEZE ALL for stability\n",
    "        backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        cnn_features = backbone.fc.in_features  # 512\n",
    "        backbone.fc = nn.Identity()\n",
    "        self.cnn = backbone\n",
    "        \n",
    "        # FREEZE entire CNN\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # CNN feature processor (trainable)\n",
    "        self.cnn_processor = nn.Sequential(\n",
    "            nn.Linear(cnn_features, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Tabular MLP - deeper network\n",
    "        self.tabular = nn.Sequential(\n",
    "            nn.Linear(tabular_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Simple concatenation fusion (more stable than attention)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(128, 64),  # 64 (img) + 64 (tab) = 128\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, img, tab):\n",
    "        # Extract frozen CNN features\n",
    "        with torch.no_grad():\n",
    "            img_feat = self.cnn(img)\n",
    "        \n",
    "        # Process CNN features\n",
    "        img_feat = self.cnn_processor(img_feat)\n",
    "        \n",
    "        # Tabular features\n",
    "        tab_feat = self.tabular(tab)\n",
    "        \n",
    "        # Concatenate and predict\n",
    "        combined = torch.cat([img_feat, tab_feat], dim=1)\n",
    "        return self.head(combined).squeeze(1)\n",
    "\n",
    "print(\"‚úÖ STABLE Model: Frozen CNN + Concatenation Fusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9712e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders - USE ORIGINAL PRICES (not log) for Hybrid model stability\n",
    "# XGBoost uses log target, but Hybrid uses original prices\n",
    "\n",
    "# For hybrid model, use original (non-log) targets\n",
    "y_train_hybrid = y_train_original.astype(np.float32)\n",
    "y_val_hybrid = y_val_original.astype(np.float32)\n",
    "\n",
    "train_dataset = PropertyDataset(X_train, y_train_hybrid, image_ids=train_image_ids, train_mode=True)\n",
    "val_dataset = PropertyDataset(X_val, y_val_hybrid, image_ids=val_image_ids, train_mode=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "print(f\"‚úÖ Hybrid model uses ORIGINAL prices (not log-transformed) for stability\")\n",
    "print(f\"   Target range: ${y_train_hybrid.min():,.0f} to ${y_train_hybrid.max():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f5433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model - STABLE configuration\n",
    "model = HybridModel(tabular_dim=X_train.shape[1]).to(DEVICE)\n",
    "\n",
    "# Count trainable vs total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.1f}%)\")\n",
    "\n",
    "# Simple optimizer - no differential LR needed since CNN is frozen\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "print(f\"‚úÖ Learning rate: 1e-3, Scheduler: ReduceLROnPlateau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b265d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop - STABLE version (no log transform for hybrid)\n",
    "best_rmse = float('inf')\n",
    "best_state = None\n",
    "history = {'train_loss': [], 'val_rmse': []}\n",
    "patience_counter = 0\n",
    "patience = 7\n",
    "\n",
    "for epoch in range(1, CONFIG['epochs'] + 1):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for img, tab, y in tqdm(train_loader, desc=f'Epoch {epoch}', leave=False):\n",
    "        img, tab, y = img.to(DEVICE), tab.to(DEVICE), y.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = model(img, tab)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * len(y)\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    # Validate\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for img, tab, y in val_loader:\n",
    "            img, tab = img.to(DEVICE), tab.to(DEVICE)\n",
    "            pred = model(img, tab)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            targets.extend(y.numpy())\n",
    "    \n",
    "    # Direct RMSE (no log transform for hybrid)\n",
    "    val_rmse = np.sqrt(mean_squared_error(targets, preds))\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_rmse'].append(val_rmse)\n",
    "    \n",
    "    scheduler.step(val_rmse)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    if val_rmse < best_rmse:\n",
    "        best_rmse = val_rmse\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        patience_counter = 0\n",
    "        print(f\"Epoch {epoch:02d} | Loss: {train_loss/1e9:.2f}B | Val RMSE: ${val_rmse:,.0f} | LR: {current_lr:.2e} ‚úì\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"Epoch {epoch:02d} | Loss: {train_loss/1e9:.2f}B | Val RMSE: ${val_rmse:,.0f} | LR: {current_lr:.2e}\")\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\n‚èπÔ∏è Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nüèÜ Best Val RMSE: ${best_rmse:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca47ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate\n",
    "model.load_state_dict(best_state)\n",
    "model.eval()\n",
    "\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for img, tab, _ in val_loader:\n",
    "        img, tab = img.to(DEVICE), tab.to(DEVICE)\n",
    "        pred = model(img, tab)\n",
    "        preds.extend(pred.cpu().numpy())\n",
    "\n",
    "# Direct evaluation (no log transform for hybrid)\n",
    "hybrid_preds = np.array(preds)\n",
    "hybrid_results = evaluate_model(y_val_original, hybrid_preds, 'Hybrid (Tabular + Satellite)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3836310b",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55090eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "results_df = pd.DataFrame([xgb_results, hybrid_results])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "display(results_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2227eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "colors = ['steelblue', 'coral']\n",
    "\n",
    "axes[0].bar(results_df['model'], results_df['RMSE'], color=colors)\n",
    "axes[0].set_ylabel('RMSE ($)')\n",
    "axes[0].set_title('RMSE (Lower is Better)')\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "axes[1].bar(results_df['model'], results_df['R2'], color=colors)\n",
    "axes[1].set_ylabel('R¬≤ Score')\n",
    "axes[1].set_title('R¬≤ (Higher is Better)')\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/model_comparison.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Improvements\n",
    "xgb_rmse = xgb_results['RMSE']\n",
    "hybrid_rmse = hybrid_results['RMSE']\n",
    "\n",
    "print(f\"\\nüìà Results Summary:\")\n",
    "print(f\"   XGBoost RMSE:  ${xgb_rmse:,.0f}\")\n",
    "print(f\"   Hybrid RMSE:   ${hybrid_rmse:,.0f} ({100*(xgb_rmse-hybrid_rmse)/xgb_rmse:+.1f}% vs XGB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b25257a",
   "metadata": {},
   "source": [
    "## 7. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4863e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset and generate XGBoost predictions\n",
    "test_image_ids = list(range(len(test_df)))\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "# Generate XGBoost predictions (convert from log scale)\n",
    "xgb_test_preds_raw = xgb_model.predict(X_test)\n",
    "if CONFIG['use_log_target']:\n",
    "    test_preds = np.expm1(xgb_test_preds_raw)  # Convert log(price) back to price\n",
    "else:\n",
    "    test_preds = xgb_test_preds_raw\n",
    "\n",
    "print(f\"\\n‚úÖ Generated {len(test_preds)} XGBoost model predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc1291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'] if 'id' in test_df.columns else range(len(test_preds)),\n",
    "    'predicted_price': test_preds\n",
    "})\n",
    "\n",
    "submission.to_csv(f'{OUTPUT_DIR}/predictions.csv', index=False)\n",
    "print(f\"‚úÖ Saved predictions to {OUTPUT_DIR}/predictions.csv\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846f6696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction statistics\n",
    "print(\"\\nüìä Prediction Statistics:\")\n",
    "print(f\"   Mean:   ${test_preds.mean():,.2f}\")\n",
    "print(f\"   Median: ${np.median(test_preds):,.2f}\")\n",
    "print(f\"   Min:    ${test_preds.min():,.2f}\")\n",
    "print(f\"   Max:    ${test_preds.max():,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0905b7",
   "metadata": {},
   "source": [
    "## 8. Grad-CAM Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90580f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "class GradCAMForFrozenCNN:\n",
    "    \"\"\"\n",
    "    Grad-CAM implementation that works with frozen CNN backbones.\n",
    "    Temporarily enables gradients for visualization only.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.handles = []\n",
    "        \n",
    "        # Register hooks\n",
    "        self.handles.append(target_layer.register_forward_hook(self._forward_hook))\n",
    "        self.handles.append(target_layer.register_full_backward_hook(self._backward_hook))\n",
    "    \n",
    "    def _forward_hook(self, module, input, output):\n",
    "        # Store activations WITH gradients enabled\n",
    "        self.activations = output\n",
    "    \n",
    "    def _backward_hook(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0]\n",
    "    \n",
    "    def generate(self, img, tab):\n",
    "        \"\"\"Generate Grad-CAM heatmap by temporarily enabling gradients.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Temporarily enable gradients for all CNN parameters\n",
    "        original_requires_grad = {}\n",
    "        for name, param in self.model.cnn.named_parameters():\n",
    "            original_requires_grad[name] = param.requires_grad\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        try:\n",
    "            # Need to override the forward to not use torch.no_grad()\n",
    "            # Extract CNN features WITH gradients\n",
    "            img_feat = self.model.cnn(img)\n",
    "            img_feat_processed = self.model.cnn_processor(img_feat)\n",
    "            tab_feat = self.model.tabular(tab)\n",
    "            combined = torch.cat([img_feat_processed, tab_feat], dim=1)\n",
    "            output = self.model.head(combined).squeeze(1)\n",
    "            \n",
    "            self.model.zero_grad()\n",
    "            output.backward(retain_graph=True)\n",
    "            \n",
    "            if self.gradients is None:\n",
    "                return None\n",
    "            \n",
    "            # Compute Grad-CAM\n",
    "            weights = self.gradients.mean(dim=(2, 3), keepdim=True)\n",
    "            cam = (weights * self.activations).sum(dim=1, keepdim=True)\n",
    "            cam = torch.relu(cam).squeeze().cpu().detach().numpy()\n",
    "            \n",
    "            # Normalize\n",
    "            if cam.max() - cam.min() > 1e-8:\n",
    "                cam = (cam - cam.min()) / (cam.max() - cam.min())\n",
    "            else:\n",
    "                cam = np.zeros_like(cam)\n",
    "            \n",
    "            return cam\n",
    "            \n",
    "        finally:\n",
    "            # Restore original requires_grad state\n",
    "            for name, param in self.model.cnn.named_parameters():\n",
    "                param.requires_grad = original_requires_grad[name]\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for handle in self.handles:\n",
    "            handle.remove()\n",
    "\n",
    "print(\"‚úÖ GradCAM class ready (works with frozen CNN)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca7bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Grad-CAM for sample properties\n",
    "def visualize_gradcam_samples(model, val_loader, val_data, n_samples=3):\n",
    "    \"\"\"Generate Grad-CAM visualizations for sample properties.\"\"\"\n",
    "    \n",
    "    # Get target layer (last conv layer of ResNet18)\n",
    "    target_layer = model.cnn.layer4[-1].conv2\n",
    "    gradcam = GradCAMForFrozenCNN(model, target_layer)\n",
    "    \n",
    "    # Get sample indices from validation set\n",
    "    sample_indices = np.random.choice(len(val_data), n_samples, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_samples, 3, figsize=(15, 5*n_samples))\n",
    "    if n_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, sample_idx in enumerate(sample_indices):\n",
    "        # Get data from dataloader\n",
    "        img_id = val_image_ids[sample_idx]\n",
    "        img_path = os.path.join(IMAGE_DIR, f'img_{int(img_id)}.png')\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Image not found: {img_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Load original image\n",
    "        original_img = Image.open(img_path).convert('RGB')\n",
    "        original_img_resized = original_img.resize((CONFIG['image_size'], CONFIG['image_size']))\n",
    "        original_np = np.array(original_img_resized)\n",
    "        \n",
    "        # Prepare tensors\n",
    "        transform = T.Compose([\n",
    "            T.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        img_tensor = transform(original_img).unsqueeze(0).to(DEVICE)\n",
    "        tab_tensor = torch.from_numpy(X_val[sample_idx:sample_idx+1].astype(np.float32)).to(DEVICE)\n",
    "        \n",
    "        # Generate Grad-CAM\n",
    "        cam = gradcam.generate(img_tensor, tab_tensor)\n",
    "        \n",
    "        if cam is None:\n",
    "            print(f\"Could not generate CAM for sample {i}\")\n",
    "            continue\n",
    "        \n",
    "        # Resize CAM to image size\n",
    "        cam_resized = cv2.resize(cam, (CONFIG['image_size'], CONFIG['image_size']))\n",
    "        \n",
    "        # Create heatmap\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Overlay\n",
    "        overlay = np.uint8(0.4 * heatmap + 0.6 * original_np)\n",
    "        \n",
    "        # Get actual price\n",
    "        actual_price = y_val_original[sample_idx]\n",
    "        \n",
    "        # Plot\n",
    "        axes[i, 0].imshow(original_np)\n",
    "        axes[i, 0].set_title(f'Satellite Image\\nPrice: \\${actual_price:,.0f}')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(cam_resized, cmap='jet')\n",
    "        axes[i, 1].set_title('Grad-CAM Heatmap')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(overlay)\n",
    "        axes[i, 2].set_title('Overlay\\n(Red = High Importance)')\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    gradcam.remove_hooks()\n",
    "    \n",
    "    plt.suptitle('Grad-CAM: What the Model Sees in Satellite Images', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{OUTPUT_DIR}/gradcam_visualization.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"‚úÖ Saved Grad-CAM visualization to {OUTPUT_DIR}/gradcam_visualization.png\")\n",
    "\n",
    "# Run visualization\n",
    "print(\"üîç Generating Grad-CAM visualizations...\")\n",
    "visualize_gradcam_samples(model, val_loader, val_data, n_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b20acc",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a42f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-generate Summary\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "xgb_rmse = xgb_results['RMSE']\n",
    "hybrid_rmse = hybrid_results['RMSE']\n",
    "\n",
    "best_model = results_df.loc[results_df['RMSE'].idxmin(), 'model']\n",
    "best_rmse = results_df['RMSE'].min()\n",
    "\n",
    "hybrid_improvement = ((xgb_rmse - hybrid_rmse) / xgb_rmse) * 100\n",
    "\n",
    "# Get top features from XGBoost\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "top_features = feature_importance.head(5)['feature'].tolist()\n",
    "\n",
    "summary_md = f\"\"\"\n",
    "### üèÜ Model Comparison Results\n",
    "\n",
    "| Model | RMSE | R¬≤ |\n",
    "|-------|------|-----|\n",
    "| XGBoost (Tabular Only) | \\\\${xgb_results['RMSE']:,.0f} | {xgb_results['R2']:.4f} |\n",
    "| **Hybrid (Tabular + Satellite)** | **\\\\${hybrid_results['RMSE']:,.0f}** | **{hybrid_results['R2']:.4f}** |\n",
    "\n",
    "### Key Findings:\n",
    "1. **Best Model:** {best_model} (RMSE: \\\\${best_rmse:,.0f})\n",
    "2. Hybrid model improvement vs XGBoost: **{hybrid_improvement:+.2f}%**\n",
    "3. Top 5 tabular features: **{', '.join(top_features)}**\n",
    "\n",
    "### Improvements Applied:\n",
    "- ‚úÖ Frozen CNN (ResNet18) + trainable fusion layers\n",
    "- ‚úÖ Log-transformed target for XGBoost\n",
    "- ‚úÖ Enhanced image augmentation (rotation, color jitter)\n",
    "\n",
    "### Files Generated:\n",
    "- `predictions.csv` - Hybrid model test predictions\n",
    "- `model_comparison.png` - Visual comparison\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(summary_md))\n",
    "print(\"\\n‚úÖ Notebook completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
